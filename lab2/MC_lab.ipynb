{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Monte Carlo\n",
    "If you want to test/submit your solution **restart the kernel, run all cells and submit the mc_autograde.py file into codegrade.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell imports %%execwritefile command (executes cell and writes it into file). \n",
    "# All cells that start with %%execwritefile should be in mc_autograde.py file after running all cells.\n",
    "from custommagics import CustomMagics\n",
    "get_ipython().register_magics(CustomMagics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%execwritefile mc_autograde.py\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7ab207a9f93cf4d3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 1. Monte Carlo Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5f0c1d608436b67b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "For the Monte Carlo Prediction we will look at the Blackjack game (Example 5.1 from the book), for which the `BlackjackEnv` is implemented in `blackjack.py`. Note that compared to the gridworld, the state is no longer a single integer, which is why we use a dictionary to represent the value function instead of a numpy array. By using `defaultdict`, each state gets a default value of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a342b69fcfdea5b2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from blackjack import BlackjackEnv\n",
    "env = BlackjackEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Monte Carlo algorithm, we no longer have transition probabilities and we need to *interact* with the environment. This means that we start an episode by using `env.reset` and send the environment actions via `env.step` to observe the reward and next observation (state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-85356add2643980e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# So let's have a look at what we can do in general with an environment...\n",
    "import gym\n",
    "?gym.Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-251b7b17c5d08a24",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# We can also look at the documentation/implementation of a method\n",
    "?env.step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?gym.spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6decb2ab83c5bcec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "??BlackjackEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ae161126d3cb1b7b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "A very simple policy for Blackjack is to *stick* if we have 20 or 21 points and *hit* otherwise. We want to know how good this policy is. This policy is *deterministic* and therefore a function that maps an observation to a single action. Technically, we can implement this as a dictionary , a function or a class with a function, where we use the last option. Moreover, it is often useful (as you will see later) to implement a function that returns  the probability $\\pi(a|s)$ for the state action pair (the probability that this policy would perform certain action in given state). We group these two functions in a policy class. To get started, let's implement this simple policy for BlackJack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9fdcb503df9cdb08",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%%execwritefile -a mc_autograde.py\n",
    "\n",
    "class SimpleBlackjackPolicy(object):\n",
    "    \"\"\"\n",
    "    A simple BlackJack policy that sticks with 20 or 21 points and hits otherwise.\n",
    "    \"\"\"\n",
    "    def get_probs(self, states, actions):\n",
    "        \"\"\"\n",
    "        This method takes a list of states and a list of actions and returns a numpy array that contains a probability\n",
    "        of perfoming action in given state for every corresponding state action pair. \n",
    "\n",
    "        Args:\n",
    "            states: a list of states.\n",
    "            actions: a list of actions.\n",
    "\n",
    "        Returns:\n",
    "            Numpy array filled with probabilities (same length as states and actions)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # So we need to determine for every input state-action pair, what the resulting policy distribution is\n",
    "        # This means that the input will be a single state and a single action per index. \n",
    "        # We then need to determine if, according to our policy, the action should be taken (prob=1) \n",
    "        # or not (prob=0)\n",
    "        \n",
    "        # state is a tuple of (player's current sum, dealer's single showing card, boolean for usable ace)\n",
    "        probs = []\n",
    "        for index, (state, action) in enumerate(zip(states, actions)):\n",
    "            chosen_action = self.sample_action(state)\n",
    "            if action == chosen_action:\n",
    "                probs.append(1)\n",
    "            else:\n",
    "                probs.append(0)\n",
    "                    \n",
    "        \n",
    "        return np.array(probs)\n",
    "    \n",
    "    def sample_action(self, state):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            state: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        if state[0] == 20 or state[0] == 21: #Now we should stick (0)\n",
    "            action = 0\n",
    "        else: # Otherwise hit\n",
    "            action = 1\n",
    "    \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-99f02e2d9b338a5b",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's check if it makes sense\n",
    "env = BlackjackEnv()\n",
    "s = env.reset()\n",
    "policy = SimpleBlackjackPolicy()\n",
    "print(\"State: {}\\nSampled Action: {}\\nProbabilities [stick, hit]: {}\".format(s, policy.sample_action(s), policy.get_probs([s,s],[0,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are multiple algorithms which require data from single episode (or multiple episodes) it is often useful to write a routine that will sample a single episode. This will save us some time later. Implement a *sample_episode* function which uses environment and policy to sample a single episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%execwritefile -a mc_autograde.py\n",
    "\n",
    "def sample_episode(env, policy):\n",
    "    \"\"\"\n",
    "    A sampling routine. Given environment and a policy samples one episode and returns states, actions, rewards\n",
    "    and dones from environment's step function and policy's sample_action function as lists.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of lists (states, actions, rewards, dones). All lists should have same length. \n",
    "        Hint: Do not include the state after the termination in the list of states.\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    done = False\n",
    "    state = env.reset() # Could also use env._get_obs(), but codegrade seems to expect this\n",
    "    while done == False:\n",
    "        states.append(state)\n",
    "    \n",
    "        action = policy.sample_action(state)\n",
    "        actions.append(action)\n",
    "    \n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "\n",
    "    return states, actions, rewards, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's sample some episodes\n",
    "env = BlackjackEnv()\n",
    "policy = SimpleBlackjackPolicy()\n",
    "for episode in range(3):\n",
    "    trajectory_data = sample_episode(env, policy)\n",
    "    print(\"Episode {}:\\nStates {}\\nActions {}\\nRewards {}\\nDones {}\\n\".format(episode,*trajectory_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0184f4c719afb98c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now implement the MC prediction algorithm (either first visit or every visit). Hint: you can use `for i in tqdm(range(num_episodes))` to show a progress bar. Use the sampling function from above to sample data from a single episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%execwritefile -a mc_autograde.py\n",
    "\n",
    "from collections import Counter\n",
    "import gym\n",
    "\n",
    "def mc_prediction(env, policy, num_episodes, discount_factor=1.0, sampling_function=sample_episode):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given policy using sampling.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        sampling_function: Function that generates data from one episode.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of current V and count of returns for each state\n",
    "    # to calculate an update.\n",
    "    V = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    \n",
    "    # YOUR CODE HERE    \n",
    "        \n",
    "    # Due to the structure of the gym environment, it is not trivial to map the entire state space\n",
    "    # so we only map the state space of the BlackJack env\n",
    "    count_zeros = False\n",
    "    if (isinstance(env.observation_space, gym.spaces.tuple_space.Tuple)):\n",
    "        if (len(env.observation_space.spaces) == 3):\n",
    "            count_zeros = True\n",
    "        \n",
    "        \n",
    "    state_tuples = [(first, second, bool(third)) for first in range(2,env.observation_space.spaces[0].n)\n",
    "                       for second in range(1,env.observation_space.spaces[1].n)\n",
    "                       for third in range(env.observation_space.spaces[2].n)]\n",
    "    returns = {state_tuple: [] for state_tuple in state_tuples}\n",
    "    \n",
    "    if count_zeros:\n",
    "        # Replace the returns_count with a Counter object, and fill with all possible states\n",
    "        returns_count = Counter({state_tuple: 0 for state_tuple in state_tuples})\n",
    "    \n",
    "    \n",
    "    for episode in tqdm(range(num_episodes)): # num_episodes\n",
    "        \n",
    "        env.reset()\n",
    "        states, actions, rewards, dones = sampling_function(env, policy)\n",
    "        p_return = 0\n",
    "        \n",
    "        for index in reversed(range(len(states))): # Reverse so we loop in opposite direction through timesteps\n",
    "            c_state = states[index]\n",
    "            c_action = actions[index]\n",
    "            c_reward = rewards[index]\n",
    "\n",
    "            p_return = discount_factor * p_return + c_reward\n",
    "                        \n",
    "            if len(returns[c_state]) == 0:\n",
    "                returns[c_state] = [p_return]\n",
    "            else:\n",
    "                returns[c_state].append(p_return)\n",
    "            \n",
    "            if count_zeros:\n",
    "                returns_count[c_state] += 1\n",
    "    \n",
    "    V = {state: np.nan_to_num(np.mean(value)) for (state, value) in returns.items()}\n",
    "    if count_zeros:\n",
    "        zero_counts = [True for item in list(returns_count) if returns_count[item] == 0]\n",
    "    \n",
    "        no_of_zero = sum(zero_counts)\n",
    "        if no_of_zero>0:\n",
    "            print(f\"Did not reach {no_of_zero} states in MC estimation. Value estimation for these states is missing.\")\n",
    "        else:\n",
    "            print(\"Reached all states in MC estimation.\")\n",
    "        \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_10k = mc_prediction(env, SimpleBlackjackPolicy(), num_episodes=10000)\n",
    "print(V_10k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9d32f907f180c088",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now make *4 plots* like Figure 5.1 in the book. You can either make 3D plots or heatmaps. Make sure that your results look similar to the results in the book. Give your plots appropriate titles, axis labels, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cbaf4d6a0e4c00fa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Let's run your code one time\n",
    "V_10k = mc_prediction(env, SimpleBlackjackPolicy(), num_episodes=10000)\n",
    "V_500k = mc_prediction(env, SimpleBlackjackPolicy(), num_episodes=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ba046443478aa517",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def get_mesh(value_dict, usable=True, plot_surf=False, ax=None):\n",
    "    \n",
    "    value_array = np.array([[state[0], state[1], value_dict[state]] for state in value_dict.keys() if state[2]==usable])\n",
    "    \n",
    "    max_y = int(np.max(value_array[:, 0])) # y axis for player sum\n",
    "    min_y = int(np.min(value_array[:, 0]))\n",
    "    max_x = int(np.max(value_array[:, 1])) # x axis for dealer value\n",
    "    min_x = int(np.min(value_array[:, 1]))\n",
    "    range_x = range(min_x, max_x+1)\n",
    "    range_y = range(min_y, max_y+1)\n",
    "    X, Y = np.meshgrid(range_x, range_y)\n",
    "    Z = np.zeros(X.shape)\n",
    "    \n",
    "    for y, x, value in value_array:\n",
    "        Z[int(y)-min_y, int(x)-min_x] = value\n",
    "        \n",
    "    \n",
    "    \n",
    "    if plot_surf:\n",
    "        if ax is None:\n",
    "            ax = plt.axes(projection='3d')\n",
    "        ax.plot_wireframe(X, Y, Z)\n",
    "        ax.set_ylim3d(12, 21)\n",
    "        ax.set_xlim3d(1,10)\n",
    "        ax.set_zlim3d(-1,1)\n",
    "        \n",
    "        return X, Y, Z, ax\n",
    "    else:\n",
    "        return X, Y, Z\n",
    "\n",
    "\n",
    "for iteration_index, V_dict in enumerate([V_10k, V_500k]):\n",
    "    for usable_index, usable in enumerate([True, False]):\n",
    "        ax = plt.subplot(2, 2, 1+iteration_index + 2*usable_index, projection='3d')\n",
    "        X, Y, Z, ax = get_mesh(V_dict, usable=usable, plot_surf=True, ax=ax)\n",
    "        if usable:\n",
    "            if iteration_index == 0:\n",
    "                ax.set_title(\"Usable ace at 10k episodes\")\n",
    "            else:\n",
    "                ax.set_title(\"Usable ace at 500k episodes\")\n",
    "        else:\n",
    "            if iteration_index == 0:\n",
    "                ax.set_title(\"No usable ace at 10k episodes\")\n",
    "            else:\n",
    "                ax.set_title(\"No usable ace at 500k episodes\")\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Off-policy Monte Carlo prediction\n",
    "In real world, it is often beneficial to learn from the experience of others in addition to your own. For example, you can probably infer that running off the cliff with a car is a bad idea if you consider what \"return\" people who have tried it received.\n",
    "\n",
    "Similarly, we can benefit from the experience of other agents in reinforcement learning. In this exercise we will use off-policy monte carlo to estimate the value function of our target policy using the experience from a different behavior policy. Our target policy will be the simple policy defined above (stick if we have *20* or *21* points) and we will use a random policy that randomly chooses to stick or hit (both with 50% probability) as a behavior policy. As a first step, implement a random BlackJack policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%execwritefile -a mc_autograde.py\n",
    "\n",
    "class RandomBlackjackPolicy(object):\n",
    "    \"\"\"\n",
    "    A random BlackJack policy.\n",
    "    \"\"\"\n",
    "    def get_probs(self, states, actions):\n",
    "        \"\"\"\n",
    "        This method takes a list of states and a list of actions and returns a numpy array that contains \n",
    "        a probability of perfoming action in given state for every corresponding state action pair. \n",
    "\n",
    "        Args:\n",
    "            states: a list of states.\n",
    "            actions: a list of actions.\n",
    "\n",
    "        Returns:\n",
    "            Numpy array filled with probabilities (same length as states and actions)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        probs = np.ones(len(states))/2\n",
    "        return probs\n",
    "    \n",
    "    def sample_action(self, state):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            state: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        action = np.random.choice(1)\n",
    "        \n",
    "        \n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if it makes sense\n",
    "env = BlackjackEnv()\n",
    "s = env.reset()\n",
    "policy = RandomBlackjackPolicy()\n",
    "print(\"State: {}\\nSampled Action: {}\\nProbabilities [stick, hit]: {}\".format(s, policy.sample_action(s), policy.get_probs([s,s],[0,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the MC prediction algorithm with ordinary importance sampling. Use the sampling function from above to sample data from a single episode.\n",
    "\n",
    "Hint: Get probs functions may be handy. You can use `for i in tqdm(range(num_episodes))` to show a progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%execwritefile -a mc_autograde.py\n",
    "import gym\n",
    "\n",
    "def mc_importance_sampling(env, behavior_policy, target_policy, num_episodes, discount_factor=1.0,\n",
    "                           sampling_function=sample_episode):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given target policy using behavior policy and ordinary importance sampling.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        behavior_policy: A policy used to collect the data.\n",
    "        target_policy: A policy which value function we want to estimate.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        sampling_function: Function that generates data from one episode.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of current V and count of returns for each state\n",
    "    # to calculate an update.\n",
    "    V = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    epsilon = 1e-6\n",
    "    \n",
    "        \n",
    "    # Due to the structure of the gym environment, it is not trivial to map the entire state space\n",
    "    # so we only map the state space of the BlackJack env\n",
    "    count_zeros = False\n",
    "    if (isinstance(env.observation_space, gym.spaces.tuple_space.Tuple)):\n",
    "        if (len(env.observation_space.spaces) == 3):\n",
    "            count_zeros = True\n",
    "        \n",
    "    state_tuples = [(first, second, bool(third)) for first in range(2,env.observation_space.spaces[0].n)\n",
    "                       for second in range(1,env.observation_space.spaces[1].n)\n",
    "                       for third in range(env.observation_space.spaces[2].n)]\n",
    "    returns = {state_tuple: [] for state_tuple in state_tuples}\n",
    "    \n",
    "    if count_zeros:\n",
    "        returns_count = Counter({state_tuple: 0 for state_tuple in state_tuples})\n",
    "    \n",
    "    for episode in tqdm(range(num_episodes)): # num_episodes\n",
    "        \n",
    "        env.reset()\n",
    "        states, actions, rewards, dones = sampling_function(env, behavior_policy)\n",
    "        p_return = 0\n",
    "        \n",
    "        pi = target_policy.get_probs(states, actions)\n",
    "        b = (behavior_policy.get_probs(states, actions) + epsilon)\n",
    "        pi_div_b = target_policy.get_probs(states, actions) / (behavior_policy.get_probs(states, actions) + epsilon)\n",
    "\n",
    "        for index in reversed(range(len(states))): # Reverse so we loop in opposite direction through timesteps\n",
    "            c_state = states[index]\n",
    "            c_action = actions[index]\n",
    "            c_reward = rewards[index]\n",
    "\n",
    "            p_return = discount_factor * p_return + c_reward\n",
    "            W = np.cumprod(pi_div_b[index:])\n",
    "            \n",
    "            p_return = W[0] * p_return\n",
    "            if len(returns[c_state]) == 0:\n",
    "                returns[c_state] = [p_return]\n",
    "            else:\n",
    "                returns[c_state].append(p_return)\n",
    "\n",
    "            if count_zeros:\n",
    "                returns_count[c_state] += 1\n",
    "    \n",
    "    V = {state: np.nan_to_num(np.mean(value)) for (state, value) in returns.items()}\n",
    "    \n",
    "    if count_zeros:\n",
    "        zero_counts = [True for item in list(returns_count) if returns_count[item] == 0]\n",
    "        no_of_zero = sum(zero_counts)\n",
    "        if no_of_zero>0:\n",
    "            print(f\"Did not reach {no_of_zero} states in MC estimation. Value estimation for these states is missing.\")\n",
    "        else:\n",
    "            print(\"Reached all states in MC estimation.\")\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_10k = mc_importance_sampling(env, RandomBlackjackPolicy(), SimpleBlackjackPolicy(), num_episodes=10000)\n",
    "print(V_10k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Let's run your code one time\n",
    "V_10k = mc_importance_sampling(env, RandomBlackjackPolicy(), SimpleBlackjackPolicy(), num_episodes=10000)\n",
    "V_500k = mc_importance_sampling(env, RandomBlackjackPolicy(), SimpleBlackjackPolicy(), num_episodes=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the V function. Do the plots look like what you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "for iteration_index, V_dict in enumerate([V_10k, V_500k]):\n",
    "    for usable_index, usable in enumerate([True, False]):\n",
    "        ax = plt.subplot(2, 2, 1+iteration_index + 2*usable_index, projection='3d')\n",
    "        X, Y, Z, ax = get_mesh(V_dict, usable=usable, plot_surf=True, ax=ax)\n",
    "        ax.set_zlim3d(-1, 2)\n",
    "        if usable:\n",
    "            if iteration_index == 0:\n",
    "                ax.set_title(\"Usable ace at 10k episodes\")\n",
    "            else:\n",
    "                ax.set_title(\"Usable ace at 500k episodes\")\n",
    "        else:\n",
    "            if iteration_index == 0:\n",
    "                ax.set_title(\"No usable ace at 10k episodes\")\n",
    "            else:\n",
    "                ax.set_title(\"No usable ace at 500k episodes\")\n",
    "        \n",
    "# The value function is quite different, since it is now between 0 and 2 rather than -1 and 1. Additionally, \n",
    "# the states with a low value in the previous evaluation are less noisy now - the low value is flat at 0.\n",
    "# The value function is also less steep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test/submit your solution **restart the kernel, run all cells and submit the mc_autograde.py file into codegrade.**"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
