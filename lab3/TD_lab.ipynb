{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Reinforcement Learning - Temporal Difference\n", "If you want to test/submit your solution **restart the kernel, run all cells and submit the td_autograde.py file into codegrade.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# This cell imports %%execwritefile command (executes cell and writes it into file). \n", "# All cells that start with %%execwritefile should be in td_autograde.py file after running all cells.\n", "from custommagics import CustomMagics\n", "get_ipython().register_magics(CustomMagics)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%execwritefile td_autograde.py\n", "import numpy as np\n", "from collections import defaultdict\n", "from tqdm import tqdm as _tqdm\n", "\n", "def tqdm(*args, **kwargs):\n", "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer"]}, {"cell_type": "code", "execution_count": null, "metadata": {"nbgrader": {"grade": false, "grade_id": "cell-fc69f22067705372", "locked": true, "schema_version": 1, "solution": false}}, "outputs": [], "source": ["%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "import sys\n", "\n", "import random\n", "import time\n", "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""]}, {"cell_type": "markdown", "metadata": {"nbgrader": {"grade": false, "grade_id": "cell-eecfd6fb626abfae", "locked": true, "schema_version": 1, "solution": false}}, "source": ["## 1. Temporal Difference (TD) learning"]}, {"cell_type": "markdown", "metadata": {"nbgrader": {"grade": false, "grade_id": "cell-21ca38ffcbe1c3ca", "locked": true, "schema_version": 1, "solution": false}}, "source": ["For the TD algorithms, we will skip the prediction algorithm and go straight for the control setting where we optimize the policy that we are using. To keep it dynamic, we will use the windy gridworld environment (Example 6.5)."]}, {"cell_type": "code", "execution_count": null, "metadata": {"nbgrader": {"grade": false, "grade_id": "cell-c046fd0377cee46d", "locked": true, "schema_version": 1, "solution": false}}, "outputs": [], "source": ["from windy_gridworld import WindyGridworldEnv\n", "env = WindyGridworldEnv()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Implement the EpsilonGreedyPolicy class which uses Q-values to sample an action."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%execwritefile -a td_autograde.py\n", "\n", "class EpsilonGreedyPolicy(object):\n", "    \"\"\"\n", "    A simple epsilon greedy policy.\n", "    \"\"\"\n", "    def __init__(self, Q, epsilon):\n", "        self.Q = Q\n", "        self.epsilon = epsilon\n", "    \n", "    def sample_action(self, obs):\n", "        \"\"\"\n", "        This method takes a state as input and returns an action sampled from this policy.  \n", "\n", "        Args:\n", "            obs: current state\n", "\n", "        Returns:\n", "            An action (int).\n", "        \"\"\"\n", "        # YOUR CODE HERE\n", "        raise NotImplementedError\n", "        return action"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now implement SARSA algorithm."]}, {"cell_type": "code", "execution_count": null, "metadata": {"nbgrader": {"grade": true, "grade_id": "cell-6b662771f3762bb1", "locked": false, "points": 2, "schema_version": 1, "solution": true}}, "outputs": [], "source": ["%%execwritefile -a td_autograde.py\n", "\n", "def sarsa(env, policy, Q, num_episodes, discount_factor=1.0, alpha=0.5):\n", "    \"\"\"\n", "    SARSA algorithm: On-policy TD control. Finds the optimal epsilon-greedy policy.\n", "    \n", "    Args:\n", "        env: OpenAI environment.\n", "        policy: A policy which allows us to sample actions with its sample_action method.\n", "        Q: Q value function, numpy array Q[s,a] -> state-action value.\n", "        num_episodes: Number of episodes to run for.\n", "        discount_factor: Gamma discount factor.\n", "        alpha: TD learning rate.\n", "        \n", "    Returns:\n", "        A tuple (Q, stats).\n", "        Q is a numpy array Q[s,a] -> state-action value.\n", "        stats is a list of tuples giving the episode lengths and returns.\n", "    \"\"\"\n", "    \n", "    # Keeps track of useful statistics\n", "    stats = []\n", "    \n", "    for i_episode in tqdm(range(num_episodes)):\n", "        i = 0\n", "        R = 0\n", "        \n", "        # YOUR CODE HERE\n", "        raise NotImplementedError\n", "        \n", "        stats.append((i, R))\n", "    episode_lengths, episode_returns = zip(*stats)\n", "    return Q, (episode_lengths, episode_returns)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def running_mean(vals, n=1):\n", "    cumvals = np.array(vals).cumsum()\n", "    return (cumvals[n:] - cumvals[:-n]) / n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Q = np.zeros((env.nS, env.nA))\n", "policy = EpsilonGreedyPolicy(Q, epsilon=0.1)\n", "Q_sarsa, (episode_lengths_sarsa, episode_returns_sarsa) = sarsa(env, policy, Q, 1000)\n", "\n", "n = 50\n", "# We will help you with plotting this time\n", "plt.plot(running_mean(episode_lengths_sarsa,n))\n", "plt.title('Episode lengths SARSA')\n", "plt.show()\n", "plt.plot(running_mean(episode_returns_sarsa,n))\n", "plt.title('Episode returns SARSA')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"nbgrader": {"grade": false, "grade_id": "cell-0eaf4b925ab3ea34", "locked": true, "schema_version": 1, "solution": false}}, "source": ["We can also learn the optimal (non-exploring) policy while using another policy to do exploration, which is where we arrive at _off-policy_ learning. In the simplest variant, we learn our own value by bootstrapping based on the action value corresponding to the best action we could take, while the exploration policy actual follows the $\\epsilon$-greedy strategy. This is known as Q-learning."]}, {"cell_type": "code", "execution_count": null, "metadata": {"nbgrader": {"grade": true, "grade_id": "cell-a87637d2e582fec0", "locked": false, "points": 1, "schema_version": 1, "solution": true}}, "outputs": [], "source": ["%%execwritefile -a td_autograde.py\n", "\n", "def q_learning(env, policy, Q, num_episodes, discount_factor=1.0, alpha=0.5):\n", "    \"\"\"\n", "    Q-Learning algorithm: Off-policy TD control. Finds the optimal greedy policy\n", "    while following an epsilon-greedy policy\n", "    \n", "    Args:\n", "        env: OpenAI environment.\n", "        policy: A behavior policy which allows us to sample actions with its sample_action method.\n", "        Q: Q value function\n", "        num_episodes: Number of episodes to run for.\n", "        discount_factor: Gamma discount factor.\n", "        alpha: TD learning rate.\n", "        \n", "    Returns:\n", "        A tuple (Q, stats).\n", "        Q is a numpy array Q[s,a] -> state-action value.\n", "        stats is a list of tuples giving the episode lengths and returns.\n", "    \"\"\"\n", "    \n", "    # Keeps track of useful statistics\n", "    stats = []\n", "    \n", "    for i_episode in tqdm(range(num_episodes)):\n", "        i = 0\n", "        R = 0\n", "        \n", "        # YOUR CODE HERE\n", "        raise NotImplementedError\n", "        \n", "        stats.append((i, R))\n", "    episode_lengths, episode_returns = zip(*stats)\n", "    return Q, (episode_lengths, episode_returns)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Q = np.zeros((env.nS, env.nA))\n", "policy = EpsilonGreedyPolicy(Q, epsilon=0.1)\n", "Q_q_learning, (episode_lengths_q_learning, episode_returns_q_learning) = q_learning(env, policy, Q, 1000)\n", "\n", "n = 50\n", "# We will help you with plotting this time\n", "plt.plot(running_mean(episode_lengths_q_learning,n))\n", "plt.title('Episode lengths Q-learning')\n", "plt.show()\n", "plt.plot(running_mean(episode_returns_q_learning,n))\n", "plt.title('Episode returns Q-learning')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Comparing episode returns during training"]}, {"cell_type": "markdown", "metadata": {"nbgrader": {"grade": false, "grade_id": "cell-9f1fcee44ba712c2", "locked": true, "schema_version": 1, "solution": false}}, "source": ["You will now compare the episode returns while learning for Q-learning and Sarsa by plotting the returns for both algorithms in a single plot, like in the book, Example 6.6. In order to be able to compare them, we will smooth the returns (e.g. plot the $n$ episode average instead)."]}, {"cell_type": "code", "execution_count": null, "metadata": {"nbgrader": {"grade": true, "grade_id": "cell-69ed62a52a44dd78", "locked": false, "points": 1, "schema_version": 1, "solution": true}}, "outputs": [], "source": ["def running_mean(vals, n=1):\n", "    cumvals = np.array(vals).cumsum()\n", "    return (cumvals[n:] - cumvals[:-n]) / n\n", "\n", "n = 100\n", "plt.plot(running_mean(episode_returns_q_learning, n))\n", "plt.plot(running_mean(episode_returns_sarsa, n))\n", "plt.title('Episode returns TD')\n", "plt.legend(['Q-Learning', 'Sarsa'])\n", "plt.gca().set_ylim([-50, 0])\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If you want to test/submit your solution **restart the kernel, run all cells and submit the td_autograde.py file into codegrade.**"]}], "metadata": {"celltoolbar": "Create Assignment", "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.3"}}, "nbformat": 4, "nbformat_minor": 2}