{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Deep Q Network\n",
    "If you want to test/submit your solution **restart the kernel, run all cells and submit the dqn_autograde.py file into codegrade.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custommagics import CustomMagics\n",
    "get_ipython().register_magics(CustomMagics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dqn_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile dqn_autograde.py\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fc69f22067705372",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fef7e20e54e6243b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 1. Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-39519f4ab05eb2a1",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/rl2020/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.envs.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env is a TimeLimit wrapper around an env, so use env.env to look into the env (but otherwise you can forget about this)\n",
    "??env.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The nice thing about the CARTPOLE is that it has very nice rendering functionality (if you are on a local environment). Let's have a look at an episode\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "done = False\n",
    "while not done:\n",
    "    obs, reward, done, _ = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    time.sleep(0.05)\n",
    "    \n",
    "env.close()  # Close the environment or you will have a lot of render screens soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2d83f70e62b99520",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Remember from the previous lab, that in order to optimize a policy we need to estimate the Q-values (e.g. estimate the *action* values). In the CartPole problem, our state is current position of the cart, the current velocity of the cart, the current (angular) position of the pole and the (angular) speed of the pole. As these are continuous variables, we have an infinite number of states (ignoring the fact that a digital computer can only represent finitely many states in finite memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0b3162496f5e6cf5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.1 Implement Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-96a86bcfa1ebc84a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We will not use the tabular approach but approximate the Q-value function by a general approximator function. We will skip the linear case and directly use a two layer Neural Network. We use [PyTorch](https://pytorch.org/) to implement the network, as this will allow us to train it easily later. We can implement a model using `torch.nn.Sequential`, but with PyTorch it is actually very easy to implement the model (e.g. the forward pass) from scratch. Now implement the `QNetwork.forward` function that uses one hidden layer with ReLU activation (no output activation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-216429a5dccf8a0e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to dqn_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile -a dqn_autograde.py\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_hidden=128):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, num_hidden)\n",
    "        self.l2 = nn.Linear(num_hidden, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.Tensor(x) # Seems like this does nothing, even when numpy gets passed into Q\n",
    "        layer_1 = self.l1(x)\n",
    "        hidden = nn.functional.relu(layer_1) # Apply activation function as function rather than later\n",
    "        output = self.l2(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-00ce108d640a5942",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's instantiate and test if it works\n",
    "num_hidden = 128\n",
    "torch.manual_seed(1)\n",
    "Q_net = QNetwork(num_hidden)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "test_model = nn.Sequential(\n",
    "    nn.Linear(4, num_hidden), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(num_hidden, 2)\n",
    ")\n",
    "\n",
    "x = torch.rand(10, 4)\n",
    "\n",
    "# If you do not need backpropagation, wrap the computation in the torch.no_grad() context\n",
    "# This saves time and memory, and PyTorch complaints when converting to numpy\n",
    "with torch.no_grad():\n",
    "    assert np.allclose(Q_net(x).numpy(), test_model(x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ca77eae2e62180cf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.2 Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2c1d117a1a75fd69",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In order to stabilize learning, we will use an experience replay to save states in and sample states from. Now implement the `push` function that adds a transition to the replay buffer, and the `sample` function that samples a (random!) batch of data, for use during training (hint: you can use the function `random.sample`). It should keep at most the maximum number of transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a3cc876e51eb157f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to dqn_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile -a dqn_autograde.py\n",
    "\n",
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, transition):\n",
    "        if len(self.memory)>self.capacity-1:\n",
    "            del self.memory[0] # Would maybe be nice to store this for the case that memory.append fails\n",
    "                               # but that requires quite extensive error handling which is not important here\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3b90135921c4da76",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([-0.02415264, -0.02596274, -0.01153027, -0.0390442 ]), 1, 1.0, array([-0.0246719 ,  0.16932264, -0.01231115, -0.33534263]), False)]\n"
     ]
    }
   ],
   "source": [
    "capacity = 10\n",
    "memory = ReplayMemory(capacity)\n",
    "\n",
    "# Sample a transition\n",
    "s = env.reset()\n",
    "a = env.action_space.sample()\n",
    "s_next, r, done, _ = env.step(a)\n",
    "\n",
    "# Push a transition\n",
    "memory.push((s, a, r, s_next, done))\n",
    "\n",
    "# Sample a batch size of 1\n",
    "print(memory.sample(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-88f67e3c051da6a9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.3 $\\epsilon$psilon greedy policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aa3c7d1b3000f697",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In order to learn a good policy, we need to explore quite a bit initially. As we start to learn a good policy, we want to decrease the exploration. As the amount of exploration using an $\\epsilon$-greedy policy is controlled by $\\epsilon$, we can define an 'exploration scheme' by writing $\\epsilon$ as a function of time. There are many possible schemes, but we will use a simple one: we will start with only exploring (so taking random actions) at iteration 0, and then in 1000 iterations linearly anneal $\\epsilon$ such that after 1000 iterations we take random (exploration) actions with 5\\% probability (forever, as you never know if the environment will change)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5789e7a792108576",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to dqn_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile -a dqn_autograde.py\n",
    "\n",
    "def get_epsilon(it):\n",
    "    # YOUR CODE HERE\n",
    "    annealing_time = 1000\n",
    "    progress = it/annealing_time\n",
    "    \n",
    "    max_eps = 1\n",
    "    min_eps = 0.05\n",
    "    epsilon = max(max_eps - (max_eps - min_eps) * progress, min_eps)\n",
    "    \n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-40e66db45e742b2e",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7faa9a1d8c10>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# So what's an easy way to check?\n",
    "plt.plot([get_epsilon(it) for it in range(5000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a8b604c9998c6c3b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now write a function of *EpsilonGreedyPolicy* class. This function takes a state and uses the Q-network to select an ($\\epsilon$-greedy) action. It should return a random action with probability epsilon. Note, you do not need to backpropagate through the model computations, so use `with torch.no_grad():` (see above for example). Note that to convert a PyTorch tensor with only 1 element (0 dimensional) to a simple python scalar (int or float), you can use the '.item()' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-878ad3a637cfb51c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to dqn_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile -a dqn_autograde.py\n",
    "\n",
    "class EpsilonGreedyPolicy(object):\n",
    "    \"\"\"\n",
    "    A simple epsilon greedy policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q, epsilon):\n",
    "        self.Q = Q\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def sample_action(self, obs):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            obs: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # So we first need to choose whether we are taking a random action or a policy action\n",
    "        if random.choices([True, False], weights=[self.epsilon, 1-self.epsilon], k=1)[0]:\n",
    "            # This means we need to make a random choice for the action to be performed\n",
    "            # The size of the output layer of Q_net is hardcoded as 2, so we will do that here too\n",
    "            return random.choice(range(2))\n",
    "        else:\n",
    "            # This means we need to use the policy network\n",
    "            obs = torch.Tensor(obs) # Stays Tensor if it was already one, becomes tensor if not\n",
    "            action = torch.argmax(self.Q(obs)).item()\n",
    "            return action\n",
    "        \n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.epsilon = epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e895338d56bee477",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "epg = EpsilonGreedyPolicy(Q_net, 0.05)\n",
    "a = epg.sample_action(s)\n",
    "assert not torch.is_tensor(a)\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "?env.step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ec5e94e0b03f8aec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.4 Training function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d1a12cc97386fe56",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now we will implement the function 'train' that samples a batch from the memory and performs a gradient step using some convenient PyTorch functionality. However, you still need to compute the Q-values for the (state, action) pairs in the experience, as well as their target (e.g. the value they should move towards). What is the target for a Q-learning update? What should be the target if `next_state` is terminal (e.g. `done`)?\n",
    "\n",
    "For computing the Q-values for the actions, note that the model returns all action values where you are only interested in a single action value. Because of the batch dimension, you can't use simple indexing, but you may want to have a look at [torch.gather](https://pytorch.org/docs/stable/torch.html?highlight=gather#torch.gather) or use [advanced indexing](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html) (numpy tutorial but works mostly the same in PyTorch). Note, you should NOT modify the function train. You can view the size of a tensor `x` with `x.size()` (similar to `x.shape` in numpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all_actions=torch.Tensor([[1, 3, 4], [2, 3, 3]])\n",
    "# actions = torch.Tensor([1, 2])\n",
    "actions = np.array([1, 2])\n",
    "# print(all_actions[range(all_actions.shape[0]), actions.tolist()].unsqueeze(dim=1))\n",
    "# torch.tensor([True, False]).dtype == torch.bool\n",
    "isinstance(torch.Tensor(actions), torch.Tensor)\n",
    "actions = torch.Tensor(actions)\n",
    "torch.Tensor(torch.Tensor(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6c45485324b40081",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to dqn_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile -a dqn_autograde.py\n",
    "\n",
    "\n",
    "def compute_q_vals(Q, states, actions=None):\n",
    "    \"\"\"\n",
    "    This method returns Q values for given state action pairs.\n",
    "    \n",
    "    Args:\n",
    "        Q: Q-net\n",
    "        states: a tensor of states. Shape: batch_size x obs_dim\n",
    "        actions: a tensor of actions. Shape: Shape: batch_size x 1\n",
    "\n",
    "    Returns:\n",
    "        A torch tensor filled with Q values. Shape: batch_size x 1.\n",
    "    \"\"\"\n",
    "    all_actions = Q(states)\n",
    "    if not actions is None:\n",
    "        Q_values = all_actions[range(all_actions.shape[0]), actions.squeeze().tolist()].unsqueeze(dim=1)\n",
    "    else: # If actions are not defined, we take the best action's Q-value\n",
    "        Q_values, _ = all_actions.max(dim=1, keepdim=True)\n",
    "        # Could be updated to include the argmax as well\n",
    "    return Q_values\n",
    "    \n",
    "def compute_targets(Q, rewards, next_states, dones, discount_factor):\n",
    "    \"\"\"\n",
    "    This method returns targets (values towards which Q-values should move).\n",
    "    \n",
    "    Args:\n",
    "        Q: Q-net\n",
    "        rewards: a tensor of actions. Shape: Shape: batch_size x 1\n",
    "        next_states: a tensor of states. Shape: batch_size x obs_dim\n",
    "        dones: a tensor of boolean done flags (indicates if next_state is terminal) Shape: batch_size x 1\n",
    "        discount_factor: discount\n",
    "    Returns:\n",
    "        A torch tensor filled with target values. Shape: batch_size x 1.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # First, we need to find the max Q-value (over actions) from the next states\n",
    "    future_Q_vals = compute_q_vals(Q, next_states)\n",
    "    \n",
    "    # if the next state is terminal, the Q-value should be zero:\n",
    "    # turns out that 'dones' is not actually a boolean tensor, but an integer tensor. Waste of memory..\n",
    "    # In codegrade, the dones are in fact boolean tensors. What a mess :')\n",
    "    if dones.dtype == torch.bool: # Use boolean operators when actual boolean tensor\n",
    "        done_tensor = ~dones\n",
    "        future_Q_vals *= done_tensor\n",
    "    else: # Must be a numerical tensor representing boolean values then\n",
    "        done_tensor = 1 - dones  \n",
    "        future_Q_vals *= done_tensor\n",
    "    \n",
    "    # With some complicated indexing tricks we could prevent the done states from passing through the Q-net\n",
    "    # but this will likely not save a significant amount of processing time\n",
    "    \n",
    "    target = rewards + discount_factor * future_Q_vals\n",
    "    \n",
    "    return target\n",
    "\n",
    "def train(Q, memory, optimizer, batch_size, discount_factor):\n",
    "    # DO NOT MODIFY THIS FUNCTION\n",
    "    \n",
    "    # don't learn without some decent experience\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "\n",
    "    # random transition batch is taken from experience replay memory\n",
    "    transitions = memory.sample(batch_size)\n",
    "    \n",
    "    # transition is a list of 4-tuples, instead we want 4 vectors (as torch.Tensor's)\n",
    "    state, action, reward, next_state, done = zip(*transitions)\n",
    "    \n",
    "    # convert to PyTorch and define types\n",
    "    state = torch.tensor(state, dtype=torch.float)\n",
    "    action = torch.tensor(action, dtype=torch.int64)[:, None]  # Need 64 bit to use them as index\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "    reward = torch.tensor(reward, dtype=torch.float)[:, None]\n",
    "    done = torch.tensor(done, dtype=torch.uint8)[:, None]  # Boolean\n",
    "    \n",
    "    # compute the q value\n",
    "    q_val = compute_q_vals(Q, state, action)\n",
    "    with torch.no_grad():  # Don't compute gradient info for the target (semi-gradient)\n",
    "        target = compute_targets(Q, reward, next_state, done, discount_factor)\n",
    "    \n",
    "    # loss is measured from error between current and newly expected Q values\n",
    "    loss = F.smooth_l1_loss(q_val, target)\n",
    "\n",
    "    # backpropagation of loss to Neural Network (PyTorch magic)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()  # Returns a Python scalar, and releases history (similar to .detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b060b822eec4282f",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6255685091018677\n"
     ]
    }
   ],
   "source": [
    "# You may want to test your functions individually, but after you do so lets see if the method train works.\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "# Simple gradient descent may take long, so we will use Adam\n",
    "optimizer = optim.Adam(Q_net.parameters(), learn_rate)\n",
    "\n",
    "# We need a larger memory, fill with dummy data\n",
    "transition = memory.sample(1)[0]\n",
    "memory = ReplayMemory(10 * batch_size)\n",
    "for i in range(batch_size):\n",
    "    memory.push(transition)\n",
    "\n",
    "# Now let's see if it works\n",
    "loss = train(Q_net, memory, optimizer, batch_size, discount_factor)\n",
    "\n",
    "print (loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3eafd0ab49103f3b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.5 Put it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-36b8a04b393d8104",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now that you have implemented the training step, you should be able to put everything together. Implement the function `run_episodes` that runs a number of episodes of DQN training. It should return the durations (e.g. number of steps) of each episode. Note: we pass the train function as an argument such that we can swap it for a different training step later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "?env.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-540a7d50ecc1d046",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to dqn_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile -a dqn_autograde.py\n",
    "\n",
    "def run_episodes(train, Q, policy, memory, env, num_episodes, batch_size, discount_factor, learn_rate):\n",
    "    \n",
    "    optimizer = optim.Adam(Q.parameters(), learn_rate)\n",
    "    \n",
    "    global_steps = 0  # Count the steps (do not reset at episode start, to compute epsilon)\n",
    "    episode_durations = []  \n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        \n",
    "        steps = 0\n",
    "        while True:\n",
    "            # So it seems like here we should sample an episode,\n",
    "            # and every step update the weights\n",
    "            \n",
    "            # So first sample an action\n",
    "            sampled_action = policy.sample_action(state)\n",
    "            \n",
    "            # Then step \n",
    "            state_tuple = env.step(sampled_action)\n",
    "            \n",
    "            # Store this transition in memory:\n",
    "            s_next, r, done, _ = state_tuple\n",
    "            memory.push((state, sampled_action, r, s_next, done))\n",
    "            state = s_next\n",
    "            \n",
    "            # Now that we have added a transition, we should try to train based on our memory\n",
    "            loss = train(Q, memory, optimizer, batch_size, discount_factor)\n",
    "            # This is like online learning, we could also only train once per episode\n",
    "            \n",
    "            steps += 1\n",
    "            global_steps += 1\n",
    "            \n",
    "            # Update epsilon\n",
    "            policy.set_epsilon(get_epsilon(global_steps))\n",
    "            \n",
    "            if done:\n",
    "                if i % 10 == 0:\n",
    "                    print(\"{2} Episode {0} finished after {1} steps\"\n",
    "                          .format(i, steps, '\\033[92m' if steps >= 195 else '\\033[99m'))\n",
    "                    print(\"epsilon: \", policy.epsilon)\n",
    "                episode_durations.append(steps)\n",
    "                #plot_durations()\n",
    "                break\n",
    "    return episode_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[99m Episode 0 finished after 13 steps with reward 13.0\n",
      "epsilon:  0.98765\n",
      "\u001b[99m Episode 10 finished after 25 steps with reward 25.0\n",
      "epsilon:  0.79765\n",
      "\u001b[99m Episode 20 finished after 15 steps with reward 15.0\n",
      "epsilon:  0.64375\n",
      "\u001b[99m Episode 30 finished after 12 steps with reward 12.0\n",
      "epsilon:  0.53545\n",
      "\u001b[99m Episode 40 finished after 48 steps with reward 48.0\n",
      "epsilon:  0.26375000000000004\n",
      "\u001b[99m Episode 50 finished after 116 steps with reward 116.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 60 finished after 152 steps with reward 152.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 70 finished after 169 steps with reward 169.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 80 finished after 176 steps with reward 176.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 90 finished after 220 steps with reward 220.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 100 finished after 202 steps with reward 202.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 110 finished after 185 steps with reward 185.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 120 finished after 170 steps with reward 170.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 130 finished after 206 steps with reward 206.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 140 finished after 153 steps with reward 153.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 150 finished after 176 steps with reward 176.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 160 finished after 154 steps with reward 154.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 170 finished after 187 steps with reward 187.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 180 finished after 257 steps with reward 257.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 190 finished after 173 steps with reward 173.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 200 finished after 174 steps with reward 174.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 210 finished after 81 steps with reward 81.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 220 finished after 194 steps with reward 194.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 230 finished after 310 steps with reward 310.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 240 finished after 179 steps with reward 179.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 250 finished after 157 steps with reward 157.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 260 finished after 219 steps with reward 219.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 270 finished after 205 steps with reward 205.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 280 finished after 327 steps with reward 327.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 290 finished after 267 steps with reward 267.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 300 finished after 202 steps with reward 202.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 310 finished after 216 steps with reward 216.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 320 finished after 201 steps with reward 201.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 330 finished after 267 steps with reward 267.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 340 finished after 186 steps with reward 186.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 350 finished after 194 steps with reward 194.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 360 finished after 229 steps with reward 229.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 370 finished after 500 steps with reward 500.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 380 finished after 242 steps with reward 242.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 390 finished after 208 steps with reward 208.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 400 finished after 337 steps with reward 337.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 410 finished after 124 steps with reward 124.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 420 finished after 215 steps with reward 215.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 430 finished after 226 steps with reward 226.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 440 finished after 35 steps with reward 35.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 450 finished after 132 steps with reward 132.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 460 finished after 145 steps with reward 145.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 470 finished after 259 steps with reward 259.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 480 finished after 230 steps with reward 230.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 490 finished after 309 steps with reward 309.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 500 finished after 279 steps with reward 279.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 510 finished after 162 steps with reward 162.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 520 finished after 196 steps with reward 196.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 530 finished after 247 steps with reward 247.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 540 finished after 200 steps with reward 200.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 550 finished after 137 steps with reward 137.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 560 finished after 80 steps with reward 80.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 570 finished after 277 steps with reward 277.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 580 finished after 65 steps with reward 65.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 590 finished after 211 steps with reward 211.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 600 finished after 51 steps with reward 51.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 610 finished after 112 steps with reward 112.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 620 finished after 193 steps with reward 193.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 630 finished after 87 steps with reward 87.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 640 finished after 230 steps with reward 230.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 650 finished after 172 steps with reward 172.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 660 finished after 111 steps with reward 111.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 670 finished after 192 steps with reward 192.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 680 finished after 182 steps with reward 182.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 690 finished after 114 steps with reward 114.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 700 finished after 70 steps with reward 70.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 710 finished after 82 steps with reward 82.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 720 finished after 152 steps with reward 152.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 730 finished after 57 steps with reward 57.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 740 finished after 213 steps with reward 213.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 750 finished after 92 steps with reward 92.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 760 finished after 152 steps with reward 152.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 770 finished after 68 steps with reward 68.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 780 finished after 136 steps with reward 136.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 790 finished after 28 steps with reward 28.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 800 finished after 181 steps with reward 181.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 810 finished after 281 steps with reward 281.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 820 finished after 13 steps with reward 13.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 830 finished after 267 steps with reward 267.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 840 finished after 162 steps with reward 162.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 850 finished after 45 steps with reward 45.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 860 finished after 135 steps with reward 135.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 870 finished after 360 steps with reward 360.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 880 finished after 86 steps with reward 86.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 890 finished after 171 steps with reward 171.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 900 finished after 144 steps with reward 144.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 910 finished after 167 steps with reward 167.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 920 finished after 271 steps with reward 271.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 930 finished after 131 steps with reward 131.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 940 finished after 231 steps with reward 231.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 950 finished after 116 steps with reward 116.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 960 finished after 248 steps with reward 248.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 970 finished after 163 steps with reward 163.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 980 finished after 172 steps with reward 172.0\n",
      "epsilon:  0.05\n",
      "\u001b[92m Episode 990 finished after 314 steps with reward 314.0\n",
      "epsilon:  0.05\n"
     ]
    }
   ],
   "source": [
    "# Let's run it!\n",
    "num_episodes = 1000\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "memory = ReplayMemory(50000)\n",
    "num_hidden = 128\n",
    "seed = 48  # This is not randomly chosen\n",
    "\n",
    "# We will seed the algorithm (before initializing QNetwork!) for reproducibility\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "Q_net = QNetwork(num_hidden)\n",
    "policy = EpsilonGreedyPolicy(Q_net, 0.05)\n",
    "episode_durations = run_episodes(train, Q_net, policy, memory, env, num_episodes, batch_size, discount_factor, learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[99m Episode 0 finished after 19 steps with reward 19.0\n",
      "epsilon:  0.98195\n",
      "\u001b[99m Episode 10 finished after 19 steps with reward 19.0\n",
      "epsilon:  0.78055\n",
      "\u001b[99m Episode 20 finished after 12 steps with reward 12.0\n",
      "epsilon:  0.63235\n",
      "\u001b[99m Episode 30 finished after 24 steps with reward 24.0\n",
      "epsilon:  0.4585\n",
      "\u001b[99m Episode 40 finished after 48 steps with reward 48.0\n",
      "epsilon:  0.08515000000000006\n",
      "\u001b[99m Episode 50 finished after 82 steps with reward 82.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 60 finished after 166 steps with reward 166.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 70 finished after 75 steps with reward 75.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 80 finished after 143 steps with reward 143.0\n",
      "epsilon:  0.05\n",
      "\u001b[99m Episode 90 finished after 173 steps with reward 173.0\n",
      "epsilon:  0.05\n"
     ]
    }
   ],
   "source": [
    "# Let's run it!\n",
    "num_episodes = 100\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "memory = ReplayMemory(10000)\n",
    "num_hidden = 128\n",
    "seed = 42  # This is not randomly chosen\n",
    "\n",
    "# We will seed the algorithm (before initializing QNetwork!) for reproducibility\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "Q_net = QNetwork(num_hidden)\n",
    "policy = EpsilonGreedyPolicy(Q_net, 0.05)\n",
    "episode_durations = run_episodes(train, Q_net, policy, memory, env, num_episodes, batch_size, discount_factor, learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-928ecc11ed5c43d8",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Episode durations per episode')"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# And see the results\n",
    "def smooth(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "plt.plot(smooth(episode_durations, 10))\n",
    "plt.title('Episode durations per episode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test/submit your solution **restart the kernel, run all cells and submit the dqn_autograde.py file into codegrade.**"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
